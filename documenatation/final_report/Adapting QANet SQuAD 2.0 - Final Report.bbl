\begin{thebibliography}{10}

\bibitem{burges2013towards}
Christopher~JC Burges.
\newblock Towards the machine comprehension of text: An essay.
\newblock {\em TechReport: MSR-TR-2013-125}, 2013.

\bibitem{chen2017reading}
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.
\newblock Reading wikipedia to answer open-domain questions.
\newblock {\em arXiv preprint arXiv:1704.00051}, 2017.

\bibitem{chollet2017xception}
Fran{\c{c}}ois Chollet.
\newblock Xception: Deep learning with depthwise separable convolutions.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1251--1258, 2017.

\bibitem{clark2017simple}
Christopher Clark and Matt Gardner.
\newblock Simple and effective multi-paragraph reading comprehension.
\newblock {\em arXiv preprint arXiv:1710.10723}, 2017.

\bibitem{dai2019transformerxl}
Zihang Dai*, Zhilin Yang*, Yiming Yang, William~W. Cohen, Jaime Carbonell,
  Quoc~V. Le, and Ruslan Salakhutdinov.
\newblock Transformer-{XL}: Language modeling with longer-term dependency,
  2019.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{hu2018read+}
Minghao Hu, Yuxing Peng, Zhen Huang, Nan Yang, Ming Zhou, et~al.
\newblock Read+ verify: Machine reading comprehension with unanswerable
  questions.
\newblock {\em arXiv preprint arXiv:1808.05759}, 2018.

\bibitem{kim2014convolutional}
Yoon Kim.
\newblock Convolutional neural networks for sentence classification.
\newblock {\em arXiv preprint arXiv:1408.5882}, 2014.

\bibitem{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock Glove: Global vectors for word representation.
\newblock In {\em Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 1532--1543, 2014.

\bibitem{peters2018deep}
Matthew~E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,
  Kenton Lee, and Luke Zettlemoyer.
\newblock Deep contextualized word representations.
\newblock {\em arXiv preprint arXiv:1802.05365}, 2018.

\bibitem{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock {\em arXiv preprint arXiv:1806.03822}, 2018.

\bibitem{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock {\em arXiv preprint arXiv:1606.05250}, 2016.

\bibitem{seo2016bidirectional}
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi.
\newblock Bidirectional attention flow for machine comprehension.
\newblock {\em arXiv preprint arXiv:1611.01603}, 2016.

\bibitem{srivastava2015highway}
Rupesh~Kumar Srivastava, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Highway networks.
\newblock {\em arXiv preprint arXiv:1505.00387}, 2015.

\bibitem{sugawara2017evaluation}
Saku Sugawara, Yusuke Kido, Hikaru Yokono, and Akiko Aizawa.
\newblock Evaluation metrics for machine reading comprehension: Prerequisite
  skills and readability.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages 806--817,
  2017.

\bibitem{vsuster2018clicr}
Simon {\v{S}}uster and Walter Daelemans.
\newblock Clicr: a dataset of clinical case reports for machine reading
  comprehension.
\newblock {\em arXiv preprint arXiv:1803.09720}, 2018.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem{wang2016machine}
Shuohang Wang and Jing Jiang.
\newblock Machine comprehension using match-lstm and answer pointer.
\newblock {\em arXiv preprint arXiv:1608.07905}, 2016.

\bibitem{wang2017gated}
Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou.
\newblock Gated self-matching networks for reading comprehension and question
  answering.
\newblock In {\em Proceedings of the 55th Annual Meeting of the Association for
  Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages 189--198,
  2017.

\bibitem{weissenborn2017making}
Dirk Weissenborn, Georg Wiese, and Laura Seiffe.
\newblock Making neural qa as simple as possible but not simpler.
\newblock {\em arXiv preprint arXiv:1703.04816}, 2017.

\bibitem{yu2018qanet}
Adams~Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad
  Norouzi, and Quoc~V Le.
\newblock Qanet: Combining local convolution with global self-attention for
  reading comprehension.
\newblock {\em arXiv preprint arXiv:1804.09541}, 2018.

\end{thebibliography}
